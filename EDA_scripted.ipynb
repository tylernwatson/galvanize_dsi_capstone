{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WHAT ARE WE YELLING ABOUT?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import community\n",
    "import operator\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import EDA as eda\n",
    "import json\n",
    "import scipy.stats as scipy\n",
    "\n",
    "sns.set(color_codes = True)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mega_df = eda.import_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This creates new columns for hashtags (other than #netneutrality), links, and mentions in the tweet data.\n",
    "# It also creates a field for text without links since the same tweets with slightly different URLs keep\n",
    "# coming up.\n",
    "\n",
    "mega_df[\"other_hashtags\"] = mega_df[\"text\"].apply(eda.get_hashtags)\n",
    "mega_df[\"links\"] = mega_df[\"text\"].apply(eda.get_links)\n",
    "mega_df[\"@s\"] = mega_df[\"text\"].apply(eda.get_mentions)\n",
    "mega_df['cleaned_text'] = mega_df['text'].apply(eda.clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'text': ['oh wow @tyler sucks', '@!$ is that', 'hey@234tjeld fuk', '@', \"suck @fcc's dick\"]}\n",
    "test_df = pd.DataFrame(data=d)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"@s\"] = test_df[\"text\"].apply(eda.get_mentions)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying guided information campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create Counter that will tally up the number of times text appears in tweets\n",
    "\n",
    "tweet_text_dict = eda.tweet_text_dict_fn(mega_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campaign_tweet_set = eda.create_campaign_tweet_set(tweet_text_dict)\n",
    "original_tweet_set = eda.create_original_tweet_set(tweet_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def info_campaign(text):\n",
    "    if text in campaign_tweet_set:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def original_tweet(text):\n",
    "    if text in original_tweet_set:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create new columns using above functions\n",
    "\n",
    "mega_df['info_campaign'] = mega_df['cleaned_text'].apply(info_campaign)\n",
    "mega_df['original_tweet'] = mega_df['cleaned_text'].apply(original_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get to work seeing if there are any tweets in here that appear to be pro-repeal of net neutrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to be used in apply - labels tweets that appear to be pro-repeal of net neutrality\n",
    "\n",
    "mega_df['pro_repeal'] = mega_df['other_hashtags'].apply(eda.negative_tweet_grab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create array of usernames that were classified as 'pro-repeal'\n",
    "\n",
    "pro_repeal_users = mega_df['user'][mega_df['pro_repeal'] == 1]\n",
    "pro_repeal_users_unique = mega_df['user'][mega_df['pro_repeal'] == 1].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now let's split the data into 3 categories:\n",
    "### * Guided information campaign tweets\n",
    "### * Original tweets (only showed up one time)\n",
    "### * Tweets that appear to be pro-repeal of net neutrality that still used #netneutrality "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campaign_tweets_df = mega_df[(mega_df['info_campaign'] == 1)]\n",
    "original_tweets_df = mega_df[(mega_df['original_tweet'] == 1) & (mega_df['pro_repeal'] == 0)]\n",
    "\n",
    "# All of the tweets in this category are determined to be original - there were not any that were classified as \\\n",
    "# pro-repeal and part of an information campaign. The total number of pro-repeal tweets is low.\n",
    "repeal_tweets_df = mega_df[(mega_df['pro_repeal'] == 1) & (mega_df['original_tweet'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_total = mega_df['fullname'].count()\n",
    "new_total = (campaign_tweets_df['fullname'].count()) + (original_tweets_df['fullname'].count()) + \\\n",
    "(repeal_tweets_df['fullname'].count())\n",
    "\n",
    "# This removed a total of 7.72% of my data since I was not confident in how to classify it as an information campaign\n",
    "# or original tweet.\n",
    "print(((original_total - new_total) / original_total) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For my final dataset, what was the breakdown between these three categories?\n",
    "total_tweets = campaign_tweets_df['fullname'].count() + repeal_tweets_df['fullname'].count() + original_tweets_df['fullname'].count()\n",
    "percentages = [str(campaign_tweets_df['fullname'].count()/total_tweets), str(repeal_tweets_df['fullname'].count()/total_tweets), \\\n",
    "         str(original_tweets_df['fullname'].count()/total_tweets)]\n",
    "\n",
    "plt.axes(aspect='equal')\n",
    "plt.pie([campaign_tweets_df['fullname'].count(), repeal_tweets_df['fullname'].count(), \\\n",
    "         original_tweets_df['fullname'].count()], explode=[.1, .1 , .1], \\\n",
    "        labels=['Campaign Tweets', 'Repeal Tweets', 'Original Tweets'], autopct='%3.2f%%')\n",
    "plt.title('Percent of Tweets in Each Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mega_df['retweets'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count of retweets with outliers removed\n",
    "\n",
    "retweets_outliers_rmv = eda.remove_outliers(mega_df, 'retweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticks = np.linspace(0,1, 11)\n",
    "\n",
    "plt.hist(retweets_outliers_rmv, alpha=.5, bins = retweets_outliers_rmv.max(), normed=True)\n",
    "plt.xlabel('Number of Retweets')\n",
    "plt.ylabel('Percent of Total')\n",
    "plt.yticks(ticks)\n",
    "plt.title('Less than 20% of all Tweets were retweeted')\n",
    "plt.xlim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count of likes with outliers removed\n",
    "\n",
    "likes_outliers_rmv = eda.remove_outliers(mega_df, 'likes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticks = np.linspace(0,1, 11)\n",
    "\n",
    "plt.hist(likes_outliers_rmv, alpha=.5, bins = likes_outliers_rmv.max(), normed=True)\n",
    "plt.xlabel('Number of Likes')\n",
    "plt.ylabel('Percent of Total')\n",
    "plt.yticks(ticks)\n",
    "plt.title('Less than 30% of all Tweets were liked')\n",
    "plt.xlim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count of replies with outliers removed\n",
    "\n",
    "replies_outliers_rmv = eda.remove_outliers(mega_df, 'replies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticks = np.linspace(0,1, 11)\n",
    "\n",
    "plt.hist(replies_outliers_rmv, bins = replies_outliers_rmv.max(), alpha=.5, normed=True)\n",
    "plt.xlabel('Number of Replies')\n",
    "plt.ylabel('Percent of Total')\n",
    "plt.yticks(ticks)\n",
    "plt.title('Less than 10% of all Tweets had replies')\n",
    "plt.xlim(0,10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to compare the different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Original tweets\n",
    "\n",
    "orig_retweets_outliers_rmv = eda.remove_outliers(original_tweets_df, 'retweets')\n",
    "\n",
    "orig_likes_outliers_rmv = eda.remove_outliers(original_tweets_df, 'likes')\n",
    "\n",
    "orig_replies_outliers_rmv = eda.remove_outliers(original_tweets_df, 'replies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Campaign tweets\n",
    "\n",
    "campaign_retweets_outliers_rmv = eda.remove_outliers(campaign_tweets_df, 'retweets')\n",
    "\n",
    "campaign_likes_outliers_rmv = eda.remove_outliers(campaign_tweets_df, 'likes')\n",
    "\n",
    "campaign_replies_outliers_rmv = eda.remove_outliers(campaign_tweets_df, 'replies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticks = np.linspace(0,1, 11)\n",
    "bins_x = orig_replies_outliers_rmv.max()\n",
    "bins_y = campaign_replies_outliers_rmv.max()\n",
    "\n",
    "x = orig_replies_outliers_rmv\n",
    "y = campaign_replies_outliers_rmv\n",
    "\n",
    "plt.hist(x, alpha=0.5, bins=bins_x, label='Replies to Original Tweets', normed=1)\n",
    "plt.hist(y, alpha=0.5, bins=bins_y, label='Replies to Campaign Tweets', normed=1)\n",
    "plt.yticks(ticks)\n",
    "plt.xlim(0, 15)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Very Few Campaign Tweets Get Replies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hypothesize that people can tell what is real and what is a campaign and they are much more likely to engage in organic conversations by replying.\n",
    "# Change y axis to log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_replies = (original_tweets_df[original_tweets_df['replies'] != 0]['fullname'].count() / len(original_tweets_df['replies'])) * 100\n",
    "campaign_replies = (campaign_tweets_df[campaign_tweets_df['replies'] != 0]['fullname'].count() / len(campaign_tweets_df['replies'])) * 100\n",
    "\n",
    "print('Of all original tweets in the data, {:0.2f}% had a reply.'.format(original_replies))\n",
    "print('Of all campaign tweets in the data, {:0.2f}% had a reply.'.format(campaign_replies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticks = np.linspace(0,1, 11)\n",
    "bins_x = orig_retweets_outliers_rmv.max()\n",
    "bins_y = campaign_retweets_outliers_rmv.max()\n",
    "x = orig_retweets_outliers_rmv\n",
    "y = campaign_retweets_outliers_rmv\n",
    "\n",
    "plt.hist(x, alpha=0.5, bins=bins_x, label='Retweets of Original Tweets', normed=1)\n",
    "plt.hist(y, alpha=0.5, bins=bins_y, label='Retweets of Campaign Tweets', normed=1)\n",
    "plt.yticks(ticks)\n",
    "plt.xlim(0, 15)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('About A Quarter of Original Tweets Were Retweeted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_retweets = (original_tweets_df[original_tweets_df['retweets'] != 0]['fullname'].count() / len(original_tweets_df['retweets'])) * 100\n",
    "campaign_retweets = (campaign_tweets_df[campaign_tweets_df['retweets'] != 0]['fullname'].count() / len(campaign_tweets_df['retweets'])) * 100\n",
    "\n",
    "print('Of all original tweets in the data, {:0.2f}% were retweeted.'.format(original_retweets))\n",
    "print('Of all campaign tweets in the data, {:0.2f}% were retweeted.'.format(campaign_retweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticks = np.linspace(0,1, 11)\n",
    "bins_x = orig_likes_outliers_rmv.max()\n",
    "bins_y = campaign_likes_outliers_rmv.max()\n",
    "x = orig_likes_outliers_rmv\n",
    "y = campaign_likes_outliers_rmv\n",
    "\n",
    "plt.hist(x, alpha=0.5, bins=bins_x, label='Likes of Original Tweets', normed=1)\n",
    "plt.hist(y, alpha=0.5, bins=bins_y, label='Likes of Campaign Tweets', normed=1)\n",
    "plt.yticks(ticks)\n",
    "plt.xlim(0, 15)\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_likes = (original_tweets_df[original_tweets_df['likes'] != 0]['fullname'].count() / len(original_tweets_df['likes'])) * 100\n",
    "campaign_likes = (campaign_tweets_df[campaign_tweets_df['likes'] != 0]['fullname'].count() / len(campaign_tweets_df['likes'])) * 100\n",
    "\n",
    "print('Of all original tweets in the data, {:0.2f}% got likes.'.format(original_likes))\n",
    "print('Of all campaign tweets in the data, {:0.2f}% got likes.'.format(campaign_likes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start doing some NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_adj_lemmas(dataframe):\n",
    "    lemma = []\n",
    "    for doc in nlp.pipe(dataframe['cleaned_text'].astype('unicode').values, batch_size=50,\n",
    "                            n_threads=3):\n",
    "        if doc.is_parsed:\n",
    "            lemma.append([n.lemma_ for n in doc if (n.pos_ == 'ADJ') & (n.lemma_ != '-PRON-')])\n",
    "        else:\n",
    "            # We want to make sure that the lists of parsed results have the\n",
    "            # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "            lemma.append(None)\n",
    "\n",
    "    dataframe['adj_lemmas'] = lemma\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mega_df_1 = mega_df[:50000]\n",
    "mega_df_2 = mega_df[50000:100000]\n",
    "mega_df_3 = mega_df[100000:150000]\n",
    "mega_df_4 = mega_df[150000:200000]\n",
    "mega_df_5 = mega_df[200000:250000]\n",
    "mega_df_6 = mega_df[250000:300000]\n",
    "mega_df_7 = mega_df[300000:350000]\n",
    "mega_df_8 = mega_df[350000:400000]\n",
    "mega_df_9 = mega_df[400000:]\n",
    "\n",
    "slice_list = [mega_df_1, mega_df_2, mega_df_3, mega_df_4, mega_df_5, mega_df_6, mega_df_7, mega_df_8, mega_df_9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mega_df_1 = get_adj_lemmas(mega_df_1)\n",
    "# mega_df_2 = get_adj_lemmas(mega_df_2)\n",
    "# mega_df_3 = get_adj_lemmas(mega_df_3)\n",
    "# mega_df_4 = get_adj_lemmas(mega_df_4)\n",
    "# mega_df_5 = get_adj_lemmas(mega_df_5)\n",
    "# mega_df_6 = get_adj_lemmas(mega_df_6)\n",
    "# mega_df_7 = get_adj_lemmas(mega_df_7)\n",
    "# mega_df_8 = get_adj_lemmas(mega_df_8)\n",
    "# mega_df_9 = get_adj_lemmas(mega_df_9)\n",
    "\n",
    "# mega_df_final = pd.concat([mega_df_1, mega_df_2, mega_df_3, mega_df_4, mega_df_5, mega_df_6, mega_df_7, mega_df_8, \\\n",
    "#                           mega_df_9], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mega_df_final.to_json('./mega_df_final.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_df_final = pd.read_json('data/mega_df_final.json')\n",
    "mega_df_final = mega_df_final.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each day recorded, what percent of tweets were original vs campaign?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mega_df_by_date = mega_df_final.set_index(\"timestamp\")\n",
    "mega_df_by_date = mega_df_by_date.resample(\"D\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(mega_df_by_date.index)\n",
    "campaigns = mega_df_by_date['info_campaign']\n",
    "originals = mega_df_by_date['original_tweet']\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "plt.subplots(figsize=(8,8))\n",
    "\n",
    "p1 = plt.bar(ind, campaigns, width, color='#d62728')\n",
    "p2 = plt.bar(ind + width, originals, width)\n",
    "\n",
    "plt.ylabel('Total Tweets')\n",
    "plt.title('Daily Counts of Campaign vs Original Tweets')\n",
    "plt.xticks(ind, (mega_df_by_date.index.date), rotation=90)\n",
    "# plt.yticks(np.arange(0, 81, 10))\n",
    "plt.legend((p1[0], p2[0]), ('Campaign Tweets', 'Original Tweets'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make data\n",
    "data = pd.DataFrame({'Campaign Tweets':mega_df_by_date['info_campaign'], 'Original Tweets':mega_df_by_date['original_tweet']})\n",
    " \n",
    "# We need to transform the data from raw data to percentage (fraction)\n",
    "data_perc = data.divide(data.sum(axis=1), axis=0)\n",
    "\n",
    "N = len(mega_df_by_date.index)\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "\n",
    "# Make the plot\n",
    "plt.subplots(figsize=(8,8))\n",
    "\n",
    "plt.stackplot(ind, data_perc['Campaign Tweets'],  data_perc['Original Tweets'], labels=['Campaign','Original'], cmap='Pastel1')\n",
    "plt.legend(loc='upper left')\n",
    "plt.margins(0,0)\n",
    "plt.xticks(ind, (mega_df_by_date.index.date), rotation=90)\n",
    "\n",
    "plt.title('Campaign/Original Tweets as a Fraction of Total by day')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at some adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "update_campaign_tweets_df = mega_df_final[mega_df_final['info_campaign'] == 1].drop_duplicates(subset='text')\n",
    "update_original_tweets_df = mega_df_final[mega_df_final['original_tweet'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "original_adj_counter = Counter()\n",
    "ignore_list = ['net', 'that', 'more', 'many', 'able', 'much', 'which', '“']\n",
    "\n",
    "for adjs in update_original_tweets_df['adj_lemmas']:\n",
    "    if (len(adjs) != 0):\n",
    "        for adj in adjs:\n",
    "            if adj not in ignore_list:\n",
    "                original_adj_counter[adj] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "campaign_adj_counter = Counter()\n",
    "ignore_list = ['net', 'that', 'more', 'many', 'able', 'much', 'which', 'fccs', 'monthsnapchat', 'powerfulinternetproviderslike', 'thiscontact']\n",
    "\n",
    "for adjs in update_campaign_tweets_df['adj_lemmas']:\n",
    "    if (len(adj) != 0):\n",
    "        for adj in adjs:\n",
    "            if adj not in ignore_list:\n",
    "                campaign_adj_counter[adj] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dict(campaign_adj_counter.most_common(10))\n",
    "\n",
    "labels, values = zip(*counts.items())\n",
    "\n",
    "# sort your values in descending order\n",
    "indSort = np.argsort(values)[::-1]\n",
    "\n",
    "# rearrange your data\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.subplots(figsize=(8,8))\n",
    "plt.bar(indexes, values, alpha=.5)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# add labels\n",
    "plt.xticks(indexes, labels)\n",
    "plt.title('Most Common Adjectives in Guided Information Campaigns')\n",
    "plt.ylabel('Appearances')\n",
    "plt.xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dict(original_adj_counter.most_common(10))\n",
    "\n",
    "labels, values = zip(*counts.items())\n",
    "\n",
    "# sort your values in descending order\n",
    "indSort = np.argsort(values)[::-1]\n",
    "\n",
    "# rearrange your data\n",
    "labels = np.array(labels)[indSort]\n",
    "values = np.array(values)[indSort]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "\n",
    "bar_width = 0.35\n",
    "\n",
    "plt.subplots(figsize=(8,8))\n",
    "plt.bar(indexes, values, alpha=.5)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# add labels\n",
    "plt.xticks(indexes, labels, rotation=45)\n",
    "plt.title('Most Common Adjectives in Original Tweets')\n",
    "plt.ylabel('Appearances')\n",
    "plt.xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting prepped to make a force-directed graph of @s\n",
    "#### First, get a set of all @s done\n",
    "#### Find every n=2 permutation of those that show up in tweets done\n",
    "#### Remove dupes done\n",
    "#### Get a count done\n",
    "#### Get them formatted like the d3 graph will need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of every 2-item combo of mentions found in tweets in the data.\n",
    "flat_list = eda.combos_of_ats(mega_df_final)\n",
    "\n",
    "# Arrange these into an array where the two mentions are arranged in alphabetical order.\n",
    "alpha_array = eda.alpha_tuples(flat_list)\n",
    "\n",
    "# Put this into a dataframe where the first item in every pair is in column 'source' and the second item is in column\n",
    "# 'target'. Create a column 'count' that has a count of every time each pair showed up in the data.\n",
    "grouped_source_target_df = eda.create_grouped_source_target(alpha_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict this to only pairs that showed up 10 times or more. The graph won't be interpretable otherwise, and this is\n",
    "# a good threshold for determining whether or not accounts were mentioned together frequenlty enough to matter.\n",
    "grouped_source_target_df = grouped_source_target_df[grouped_source_target_df['count'] > 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an index of all the unique mentions in the data.\n",
    "unique_ats = pd.Index(grouped_source_target_df['source']\n",
    "                      .append(grouped_source_target_df['target'])\n",
    "                      .reset_index(drop=True).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of every edge - a time that two accounts were mentioned in the same tweet.\n",
    "links_list = eda.create_links_list(grouped_source_target_df, unique_ats)\n",
    "\n",
    "# Create a list of every node\n",
    "nodes_list = eda.create_nodes_list(unique_ats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create this as a NetworkX graph object so I can do some community detection\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes to the graph\n",
    "for node in nodes_list:\n",
    "    G.add_node(node['index'], name=node['name'])\n",
    "\n",
    "# Add links to the graph\n",
    "for link in links_list:\n",
    "    G.add_edge(link['source'], link['target'], weight=link['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized communities using Louvain modularity\n",
    "communities = community.best_partition(G, partition=None, weight='weight', resolution=1.0, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add community numbers to the nodes\n",
    "for idx, group in enumerate(communities.values()):\n",
    "    nodes_list[idx]['group'] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_prep = {\"nodes\":nodes_list, \"links\":links_list}\n",
    "json_dump = json.dumps(json_prep, indent=1, sort_keys=True)\n",
    "\n",
    "filename_out = 'nodes_edges.json'\n",
    "json_out = open(filename_out,'w')\n",
    "json_out.write(json_dump)\n",
    "json_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_dict = nx.betweenness_centrality(G, weight='weight')\n",
    "sorted_bc_tups = sorted(bc_dict.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove highest betweened node and redo the graph to see if other communities come up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of every 2-item combo of mentions found in tweets in the data that DO NOT include the node with\n",
    "# highest betweenness centrality ('@ajitpaifcc').\n",
    "\n",
    "flat_list_one_removed = []\n",
    "\n",
    "for item in flat_list:\n",
    "    if nodes_list[sorted_bc_tups[0][0]]['name'] not in item:\n",
    "        flat_list_one_removed.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrange these into an array where the two mentions are arranged in alphabetical order.\n",
    "alpha_array_one_removed = eda.alpha_tuples(flat_list_one_removed)\n",
    "\n",
    "# Put this into a dataframe where the first item in every pair is in column 'source' and the second item is in column\n",
    "# 'target'. Create a column 'count' that has a count of every time each pair showed up in the data.\n",
    "grouped_source_target_df_one_removed = eda.create_grouped_source_target(alpha_array_one_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_source_target_df_one_removed = grouped_source_target_df_one_removed[grouped_source_target_df_one_removed['count'] > 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an index of all the unique mentions in the data.\n",
    "unique_ats_one_removed = pd.Index(grouped_source_target_df_one_removed['source']\n",
    "                      .append(grouped_source_target_df_one_removed['target'])\n",
    "                      .reset_index(drop=True).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of every edge - a time that two accounts were mentioned in the same tweet.\n",
    "links_list_one_removed = eda.create_links_list(grouped_source_target_df_one_removed, unique_ats_one_removed)\n",
    "\n",
    "# Create a list of every node\n",
    "nodes_list_one_removed = eda.create_nodes_list(unique_ats_one_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_one_removed = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for node in nodes_list_one_removed:\n",
    "    G_one_removed.add_node(node['index'], name=node['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for link in links_list_one_removed:\n",
    "    G_one_removed.add_edge(link['source'], link['target'], weight=link['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "communities_one_removed = community.best_partition(G_one_removed, partition=None, weight='weight', resolution=1.0, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, group in enumerate(communities_one_removed.values()):\n",
    "    nodes_list_one_removed[idx]['group'] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_prep_removed = {\"nodes\":nodes_list_one_removed, \"links\":links_list_one_removed}\n",
    "json_dump_removed = json.dumps(json_prep_removed, indent=1, sort_keys=True)\n",
    "\n",
    "filename_out = 'nodes_edges_one_removed.json'\n",
    "json_out = open(filename_out,'w')\n",
    "json_out.write(json_dump_removed)\n",
    "json_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove second highest betweened node and redo the graph to see if other communities come up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of every 2-item combo of mentions found in tweets in the data that DO NOT include the two nodes with\n",
    "# highest betweenness centrality ('@ajitpaifcc').\n",
    "\n",
    "flat_list_two_removed = []\n",
    "\n",
    "for item in flat_list:\n",
    "    if (nodes_list[sorted_bc_tups[0][0]]['name'] not in item) & (nodes_list[sorted_bc_tups[1][0]]['name'] not in item):\n",
    "        flat_list_two_removed.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Arrange these into an array where the two mentions are arranged in alphabetical order.\n",
    "alpha_array_two_removed = eda.alpha_tuples(flat_list_two_removed)\n",
    "\n",
    "# Put this into a dataframe where the first item in every pair is in column 'source' and the second item is in column\n",
    "# 'target'. Create a column 'count' that has a count of every time each pair showed up in the data.\n",
    "grouped_source_target_df_two_removed = eda.create_grouped_source_target(alpha_array_two_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_source_target_df_two_removed = grouped_source_target_df_two_removed[grouped_source_target_df_two_removed['count'] > 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create an index of all the unique mentions in the data.\n",
    "unique_ats_two_removed = pd.Index(grouped_source_target_df_two_removed['source']\n",
    "                      .append(grouped_source_target_df_two_removed['target'])\n",
    "                      .reset_index(drop=True).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a list of every edge - a time that two accounts were mentioned in the same tweet.\n",
    "links_list_two_removed = eda.create_links_list(grouped_source_target_df_two_removed, unique_ats_two_removed)\n",
    "\n",
    "# Create a list of every node\n",
    "nodes_list_two_removed = eda.create_nodes_list(unique_ats_two_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_two_removed = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for node in nodes_list_two_removed:\n",
    "    G_two_removed.add_node(node['index'], name=node['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for link in links_list_two_removed:\n",
    "    G_two_removed.add_edge(link['source'], link['target'], weight=link['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "communities_two_removed = community.best_partition(G_two_removed, partition=None, weight='weight', resolution=1.0, randomize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx, group in enumerate(communities_two_removed.values()):\n",
    "    nodes_list_two_removed[idx]['group'] = group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json_prep_2_removed = {\"nodes\":nodes_list_two_removed, \"links\":links_list_two_removed}\n",
    "json_dump_2_removed = json.dumps(json_prep_2_removed, indent=1, sort_keys=True)\n",
    "\n",
    "filename_out = 'nodes_edges_two_removed.json'\n",
    "json_out = open(filename_out,'w')\n",
    "json_out.write(json_dump_2_removed)\n",
    "json_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a time window (12 hours is probably good) and determine the probabilities on a gamma distro of each user showing up over that period (will need to use Bayesian modeling to update priors). Set a threshold above which I will consider something to be showing up more than expected. When this happens, look at the communities that are being detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Slice the data up into 12 hour windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the main dataframe. Figure out how many total days there are. Make the timestamp the index so this\n",
    "# can be sliced easily.\n",
    "\n",
    "mega_df_12h_slice = mega_df_final\n",
    "num_days = len(mega_df_12h_slice['timestamp'].dt.day.unique())\n",
    "mega_df_12h_slice = mega_df_12h_slice.set_index(['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...but not too easily. Slice the dataframe into 12 hour windows manually.\n",
    "\n",
    "day_1_am_df = mega_df_12h_slice.loc['2017-11-28 00:00:01':'2017-11-28 12:00:00']\n",
    "day_1_pm_df = mega_df_12h_slice.loc['2017-11-28 12:00:01':'2017-11-29 00:00:00']\n",
    "day_2_am_df = mega_df_12h_slice.loc['2017-11-29 00:00:01':'2017-11-29 12:00:00']\n",
    "day_2_pm_df = mega_df_12h_slice.loc['2017-11-29 12:00:01':'2017-11-30 00:00:00']\n",
    "day_3_am_df = mega_df_12h_slice.loc['2017-11-30 00:00:01':'2017-11-30 12:00:00']\n",
    "day_3_pm_df = mega_df_12h_slice.loc['2017-11-30 12:00:01':'2017-12-01 00:00:00']\n",
    "day_4_am_df = mega_df_12h_slice.loc['2017-12-01 00:00:01':'2017-12-01 12:00:00']\n",
    "day_4_pm_df = mega_df_12h_slice.loc['2017-12-01 12:00:01':'2017-12-02 00:00:00']\n",
    "day_5_am_df = mega_df_12h_slice.loc['2017-12-02 00:00:01':'2017-12-02 12:00:00']\n",
    "day_5_pm_df = mega_df_12h_slice.loc['2017-12-02 12:00:01':'2017-12-03 00:00:00']\n",
    "day_6_am_df = mega_df_12h_slice.loc['2017-12-03 00:00:01':'2017-12-03 12:00:00']\n",
    "day_6_pm_df = mega_df_12h_slice.loc['2017-12-03 12:00:01':'2017-12-04 00:00:00']\n",
    "day_7_am_df = mega_df_12h_slice.loc['2017-12-04 00:00:01':'2017-12-04 12:00:00']\n",
    "day_7_pm_df = mega_df_12h_slice.loc['2017-12-04 12:00:01':'2017-12-05 00:00:00']\n",
    "day_8_am_df = mega_df_12h_slice.loc['2017-12-05 00:00:01':'2017-12-05 12:00:00']\n",
    "day_8_pm_df = mega_df_12h_slice.loc['2017-12-05 12:00:01':'2017-12-06 00:00:00']\n",
    "day_9_am_df = mega_df_12h_slice.loc['2017-12-06 00:00:01':'2017-12-06 12:00:00']\n",
    "day_9_pm_df = mega_df_12h_slice.loc['2017-12-06 12:00:01':'2017-12-07 00:00:00']\n",
    "day_10_am_df = mega_df_12h_slice.loc['2017-12-07 00:00:01':'2017-12-07 12:00:00']\n",
    "day_10_pm_df = mega_df_12h_slice.loc['2017-12-07 12:00:01':'2017-12-08 00:00:00']\n",
    "day_11_am_df = mega_df_12h_slice.loc['2017-12-08 00:00:01':'2017-12-08 12:00:00']\n",
    "day_11_pm_df = mega_df_12h_slice.loc['2017-12-08 12:00:01':'2017-12-09 00:00:00']\n",
    "day_12_am_df = mega_df_12h_slice.loc['2017-12-09 00:00:01':'2017-12-09 12:00:00']\n",
    "day_12_pm_df = mega_df_12h_slice.loc['2017-12-09 12:00:01':'2017-12-10 00:00:00']\n",
    "day_13_am_df = mega_df_12h_slice.loc['2017-12-10 00:00:01':'2017-12-10 12:00:00']\n",
    "day_13_pm_df = mega_df_12h_slice.loc['2017-12-10 12:00:01':'2017-12-11 00:00:00']\n",
    "day_14_am_df = mega_df_12h_slice.loc['2017-12-11 00:00:01':'2017-12-11 12:00:00']\n",
    "day_14_pm_df = mega_df_12h_slice.loc['2017-12-11 12:00:01':'2017-12-12 00:00:00']\n",
    "day_15_am_df = mega_df_12h_slice.loc['2017-12-12 00:00:01':'2017-12-12 12:00:00']\n",
    "day_15_pm_df = mega_df_12h_slice.loc['2017-12-12 12:00:01':'2017-12-13 00:00:00']\n",
    "day_16_am_df = mega_df_12h_slice.loc['2017-12-13 00:00:01':'2017-12-13 12:00:00']\n",
    "day_16_pm_df = mega_df_12h_slice.loc['2017-12-13 12:00:01':'2017-12-14 00:00:00']\n",
    "day_17_am_df = mega_df_12h_slice.loc['2017-12-14 00:00:01':'2017-12-14 12:00:00']\n",
    "day_17_pm_df = mega_df_12h_slice.loc['2017-12-14 12:00:01':'2017-12-15 00:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419343\n",
      "419343\n"
     ]
    }
   ],
   "source": [
    "# Confirming that this contains the same number of entries total as the original dataframe.\n",
    "\n",
    "print(len(day_1_am_df['user']) + len(day_1_pm_df['user']) + len(day_2_am_df['user']) + len(day_2_pm_df['user']) \\\n",
    "+ len(day_3_am_df['user']) + len(day_3_pm_df['user']) + len(day_4_am_df['user']) + len(day_4_pm_df['user'])\\\n",
    "+ len(day_5_am_df['user']) + len(day_5_pm_df['user']) + len(day_6_am_df['user']) + len(day_6_pm_df['user'])\\\n",
    "+ len(day_7_am_df['user']) + len(day_7_pm_df['user']) + len(day_8_am_df['user']) + len(day_8_pm_df['user'])\\\n",
    "+ len(day_9_am_df['user']) + len(day_9_pm_df['user']) + len(day_10_am_df['user']) + len(day_10_pm_df['user'])\\\n",
    "+ len(day_11_am_df['user']) + len(day_11_pm_df['user']) + len(day_12_am_df['user']) + len(day_12_pm_df['user'])\\\n",
    "+ len(day_13_am_df['user']) + len(day_13_pm_df['user']) + len(day_14_am_df['user']) + len(day_14_pm_df['user'])\\\n",
    "+ len(day_15_am_df['user']) + len(day_15_pm_df['user']) + len(day_16_am_df['user']) + len(day_16_pm_df['user'])\\\n",
    "+ len(day_17_am_df['user']) + len(day_17_pm_df['user']))\n",
    "\n",
    "print(len(mega_df_final['user']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of all unique users that are mentioned in tweets.\n",
    "\n",
    "mentioned_set = set()\n",
    "\n",
    "for item in mega_df_final['@s']:\n",
    "    for name in item:\n",
    "        mentioned_set.add(name)\n",
    "        \n",
    "mentioned_array = np.array(list(mentioned_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above array to create a dataframe of all unique users who are mentioned in tweets with a 'frequency' column\n",
    "# to be filled in with the probability each user will be mentioned in any randomly selected tweet.\n",
    "\n",
    "user_mention_freq_df = pd.DataFrame(mentioned_array, columns=['user'])\n",
    "user_mention_freq_df['prob_of_appearing'] = pd.Series(np.zeros((len(mentioned_set), )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a counter dictionary that tells us how many times every unique user was @d in a tweet.\n",
    "\n",
    "at_counter = Counter()\n",
    "\n",
    "for at in mega_df_final['@s']:\n",
    "    for item in at:\n",
    "        at_counter[item] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the 'prob_of_appearing' column.\n",
    "\n",
    "total_tweets = mega_df_final['text'].count()\n",
    "\n",
    "for i, user in enumerate(user_mention_freq_df['user']):\n",
    "    user_mention_freq_df.at[i, 'prob_of_appearing'] = (at_counter[user] / total_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a 'prob_12h_slice' column that has this value for each user (which will become lambda).\n",
    "\n",
    "user_mention_freq_df['prob_12h_slice'] = user_mention_freq_df['prob_of_appearing'] / (num_days / (1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slice_df_list = [day_1_am_df\n",
    "day_1_pm_df, day_2_am_df, day_2_pm_df, day_3_am_df, day_3_pm_df, day_4_am_df, day_4_pm_df,\\\n",
    "day_5_am_df, day_5_pm_df, day_6_am_df, day_6_pm_df, day_7_am_df, day_7_pm_df, day_8_am_df, day_8_pm_df, day_9_am_df,\\\n",
    "day_9_pm_df, day_10_am_df, day_10_pm_df, day_11_am_df, day_11_pm_df, day_12_am_df, day_12_pm_df, day_13_am_df,\\\n",
    "day_13_pm_df, day_14_am_df, day_14_pm_df, day_15_am_df, day_15_pm_df, day_16_am_df, day_16_pm_df, day_17_am_df, day_17_pm_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_at_counters = []\n",
    "\n",
    "for slice_df in slice_df_list:\n",
    "    at_counter = Counter()\n",
    "    for at in slice_df['@s']:\n",
    "        for item in at:\n",
    "            at_counter[item] += 1\n",
    "    list_of_at_counters.append(at_counter)\n",
    "    \n",
    "len(list_of_at_counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name in slice_df_list:\n",
    "    print(str(name.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "slice_df_list_text = [str(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correct poisson format: 2 = k, .1 = lambda\n",
    "\n",
    "scipy.poisson.pmf(2, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_columns = ['day_1_am_ats', 'day_1_am_ats_prob', 'day_1_pm_ats', 'day_1_pm_ats_prob',\\\n",
    "'day_2_am_ats', 'day_2_am_ats_prob', 'day_2_pm_ats', 'day_2_pm_ats_prob', 'day_3_am_ats', 'day_3_am_ats_prob', 'day_3_pm_ats',\\\n",
    "'day_3_pm_ats_prob', 'day_4_am_ats', 'day_4_am_ats_prob', 'day_4_pm_ats', 'day_4_pm_ats_prob', 'day_5_am_ats', 'day_5_am_ats_prob',\\\n",
    "'day_5_pm_ats', 'day_5_pm_ats_prob', 'day_6_am_ats', 'day_6_am_ats_prob', 'day_6_pm_ats', 'day_6_pm_ats_prob', 'day_7_am_ats',\\\n",
    "'day_7_am_ats_prob', 'day_7_pm_ats', 'day_7_pm_ats_prob', 'day_8_am_ats', 'day_8_am_ats_prob', 'day_8_pm_ats', 'day_8_pm_ats_prob',\\\n",
    "'day_9_am_ats', 'day_9_am_ats_prob', 'day_9_pm_ats', 'day_9_pm_ats_prob', 'day_10_am_ats', 'day_10_am_ats_prob',\\\n",
    "'day_10_pm_ats', 'day_10_pm_ats_prob', 'day_11_am_ats', 'day_11_am_ats_prob', 'day_11_pm_ats', 'day_11_pm_ats_prob',\\\n",
    "'day_12_am_ats', 'day_12_am_ats_prob', 'day_12_pm_ats', 'day_12_pm_ats_prob', 'day_13_am_ats', 'day_13_am_ats_prob',\\\n",
    "'day_13_pm_ats', 'day_13_pm_ats_prob', 'day_14_am_ats', 'day_14_am_ats_prob', 'day_14_pm_ats', 'day_14_pm_ats_prob',\\\n",
    "'day_15_am_ats', 'day_15_am_ats_prob', 'day_15_pm_ats', 'day_15_pm_ats_prob', 'day_16_am_ats', 'day_16_am_ats_prob',\\\n",
    "'day_16_pm_ats', 'day_16_pm_ats_prob', 'day_17_am_ats', 'day_17_am_ats_prob', 'day_17_pm_ats', 'day_17_pm_ats_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in new_columns:\n",
    "    user_mention_freq_df[name] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Katie/anaconda/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "for idx, user in enumerate(user_mention_freq_df['user']):\n",
    "#     at_counter = Counter()\n",
    "#     for at in mega_df_final['@s']:\n",
    "#         for item in at:\n",
    "#             at_counter[item] += 1\n",
    "    for column in user_mention_freq_df.columns[3:71:2]:\n",
    "        user_mention_freq_df.iloc[idx][column] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "day_1_am_df\n",
    "day_1_pm_df\n",
    "day_2_am_df\n",
    "day_2_pm_df\n",
    "day_3_am_df\n",
    "day_3_pm_df\n",
    "day_4_am_df\n",
    "day_4_pm_df\n",
    "day_5_am_df\n",
    "day_5_pm_df\n",
    "day_6_am_df\n",
    "day_6_pm_df\n",
    "day_7_am_df\n",
    "day_7_pm_df\n",
    "day_8_am_df\n",
    "day_8_pm_df\n",
    "day_9_am_df\n",
    "day_9_pm_df\n",
    "day_10_am_df\n",
    "day_10_pm_df\n",
    "day_11_am_df\n",
    "day_11_pm_df\n",
    "day_12_am_df\n",
    "day_12_pm_df\n",
    "day_13_am_df\n",
    "day_13_pm_df\n",
    "day_14_am_df\n",
    "day_14_pm_df\n",
    "day_15_am_df\n",
    "day_15_pm_df\n",
    "day_16_am_df\n",
    "day_16_pm_df\n",
    "day_17_am_df\n",
    "day_17_pm_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
